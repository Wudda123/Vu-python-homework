{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uixHOa-ibm_"
   },
   "source": [
    "Python for Data Analysis, GMC, Vilnius University, 2025\n",
    "\n",
    "# HW3: Exploratory Data Analysis (EDA)\n",
    "\n",
    "- **Tasks in this homework are built around this file:** https://github.com/Tallivm/vu-python/raw/refs/heads/main/plantdata.npy. Download it beforehand and put into the same folder as your Notebook.\n",
    "- You will need to import these packages: `numpy`, `pandas`, `matplotlib`, `urllib`, `json`. You are also allowed to import `tqdm` and `itertools`. Do not import any other packages.\n",
    "- **You will need to upload your solutions into your Github repository** dedicated for the Python for Data Analysis course. Either make it public, or keep private but add Taisija as a collaborator (nickname: Tallivm).\n",
    "- Write at least one custom function somewhere, but it is not required to put everything into functions. Do not write docstrings (function description comments).\n",
    "- Keep prints informative.\n",
    "- Do not create classes (you shouldn't know yet what are those).\n",
    "- Do not change assert statements.\n",
    "\n",
    "There are 5 tasks in this Notebook. They have slightly different numbers of points between them, with subpoints shown for each subtask e.g. (0.2p). You need to collect 8 points in total to get the maximum grade.\n",
    "\n",
    "As previously, each task consists of a text cell with task description, a code cell to solve the task, and a code cell with `assert` statements to check your code for *some* possible errors.\n",
    "\n",
    "Don't hesitate to contact me or Martynas if you are stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S16-CgGtiJzX"
   },
   "outputs": [],
   "source": [
    "# your imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib import cm\n",
    "import urllib.request\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEGJFQoMRsvN"
   },
   "source": [
    "# üîÉ Task 1 (1 point): Load and examine the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4oN9IqUoizE"
   },
   "source": [
    "There is a lab called SNAILAB which studies various snails and their secretion as a cosmetic and pharmaceutical ingredient. SNAILAB researchers know that properties of snail secretion depend on what snails ate in last days. As snails aren't very fast animals, they tend to sit on plants they eat, meaning researchers can take plant samples directly under snails to research their diets.\n",
    "\n",
    "The \"plantdata.npy\" file is a Numpy array containing some of those measurements.\n",
    "- (0.1p) Load it into this Notebook as `raw_data`.\n",
    "- (0.1p) Print out the shape and type of data.\n",
    "- (0.4p) Calculate and save the total number of missing values as `n_missing`. Print out the ratio of total missing values in data.\n",
    "- (0.4p) For each **numeric** data column, print out the minimum, average, median, and maximum values. Ignore missing values.\n",
    "\n",
    "*Do not use Pandas for Task 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-kTgx3YSoabQ"
   },
   "outputs": [],
   "source": [
    "# your code\n",
    "filename = \"plantdata.npy\"\n",
    "try:\n",
    "    raw_data = np.load(filename, allow_pickle=True)\n",
    "    \n",
    "    if not np.issubdtype(raw_data.dtype, np.number):\n",
    "        raise ValueError(\"raw_data contains non-numeric columns\")\n",
    "        \n",
    "    print(f\"Data shape: {raw_data.shape}\")\n",
    "    print(f\"Data type: {raw_data.dtype}\")\n",
    "    \n",
    "    def count_missing_values(data):\n",
    "        missing_mask = np.isnan(data)\n",
    "        return np.sum(missing_mask)\n",
    "    \n",
    "    n_missing = count_missing_values(raw_data)\n",
    "    total_elements = raw_data.size\n",
    "    missing_ratio = n_missing / total_elements\n",
    "    \n",
    "    print(f\"Ratio of missing values: {missing_ratio:.4f} ({missing_ratio*100:.2f}%)\")\n",
    "    \n",
    "    print(\"\\nColumn Statistics:\")\n",
    "    print(\"Column | Min | Average | Median | Max\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for col in range(raw_data.shape[1]):\n",
    "        column_data = raw_data[:, col]\n",
    "        \n",
    "        valid_data = column_data[~np.isnan(column_data)]\n",
    "        \n",
    "        if len(valid_data) > 0:\n",
    "            min_val = np.min(valid_data)\n",
    "            avg_val = np.mean(valid_data)\n",
    "            median_val = np.median(valid_data)\n",
    "            max_val = np.max(valid_data)\n",
    "            \n",
    "            print(f\"{col:6} | {min_val:8.4f} | {avg_val:8.4f} | {median_val:8.4f} | {max_val:8.4f}\")\n",
    "        else:\n",
    "            print(f\"{col:6} | All values missing\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: File '{filename}' not found in the current directory!\")\n",
    "    print(\"Please make sure the file is in your working folder.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dBk5yN7EeCw"
   },
   "outputs": [],
   "source": [
    "assert type(raw_data) == np.ndarray\n",
    "assert n_missing == 20873"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQpSUb9cojhq"
   },
   "source": [
    "# üëç Task 2 (1 point): Make data more convenient to work with\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Azwj7RAORvRx"
   },
   "source": [
    "Because the dataset is a 2D table, it should be much easier to work with it using Pandas.\n",
    "- (0.1p) Create a DataFrame containing data from the Numpy array. Name it `raw_df`.\n",
    "- (0.2p) Using `urllib`, load a data sheet for this dataset, which is a JSON file (https://raw.githubusercontent.com/Tallivm/vu-python/refs/heads/main/plant_datasheet.json). Name the result `datasheet`.\n",
    "- (0.3p) Create a copy of `raw_df` named `named_df`. Use `datasheet` to correctly name `named_df` columns.\n",
    "- (0.4p) Use the \"mapping\" keyword from `datasheet` to replace values in certain columns of `named_df` into strings. Don't write names of data columns directly in Notebook - make it automatic. Make sure `raw_df` stays unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwvzJnbqoj2C"
   },
   "outputs": [],
   "source": [
    "# your code\n",
    "raw_df = pd.DataFrame(raw_data)\n",
    "print(\"Created raw_df DataFrame\")\n",
    "url = \"https://raw.githubusercontent.com/Tallivm/vu-python/refs/heads/main/plant_datasheet.json\"\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    datasheet = json.loads(response.read().decode('utf-8'))\n",
    "\n",
    "named_df = raw_df.copy()\n",
    "column_names = datasheet.get('column names', [])\n",
    "if column_names:\n",
    "    named_df.columns = column_names\n",
    "else:\n",
    "    print(\"Warning: No column names found in datasheet\")\n",
    "\n",
    "def apply_value_mappings(df, mapping_dict):\n",
    "    for column_name, value_mapping in mapping_dict.items():\n",
    "        if column_name in df.columns:\n",
    "            reversed_mapping = {v: k for k, v in value_mapping.items()}\n",
    "            df[column_name] = df[column_name].map(reversed_mapping)\n",
    "            print(f\"Applied mappings to column: {column_name}\")\n",
    "    return df\n",
    "\n",
    "mappings = datasheet.get('mapping', {})\n",
    "if mappings:\n",
    "    named_df = apply_value_mappings(named_df, mappings)\n",
    "    print(\"Applied all value mappings\")\n",
    "else:\n",
    "    print(\"No mappings found in datasheet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ce0QFa5sFj1d"
   },
   "outputs": [],
   "source": [
    "assert type(raw_df) == pd.DataFrame\n",
    "assert type(named_df) == pd.DataFrame\n",
    "assert type(datasheet) == dict\n",
    "assert raw_df.columns[0] == 0\n",
    "assert named_df.columns[0] == 'plant_height'\n",
    "assert named_df.loc[1, 'petal_color'] == 'red'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqXqOwFRokNe"
   },
   "source": [
    "# üîé Task 3 (1 point): Detect problematic values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiFbyUIqS3WA"
   },
   "source": [
    "- (0.1p) Print out the number of missing values for each column in `named_df`.\n",
    "- (0.1p) For each **categorical** column, print out its unique values.\n",
    "- (0.3p) Plot and show a boxplot for each **numeric** column. It should be a single boxplot with all feature \"boxes\" side-by-side. Make sure all labels are readable and nothing overlaps on the plot.\n",
    "- (0.5p) Using the IQR approach, count and print out the number of outliers in each **numeric** column. Save the total number of outliers into a variable `n_outliers_total`.\n",
    "   - Any data point below $Q1 - 1.5 \\text{IQR}$ or above $Q3 + 1.5 \\text{IQR}$ can be considered an outlier.\n",
    "   - $Q1$ is the 25% percentile, $Q3$ is the 75% percentile.\n",
    "   - $\\text{IQR} = Q3-Q1$.\n",
    "   - Ignore missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kNVhu74On-Vm"
   },
   "outputs": [],
   "source": [
    "# your code\n",
    "print(\"Missing values per column:\")\n",
    "for col in named_df.columns:\n",
    "    n_missing = named_df[col].isna().sum()\n",
    "    print(f\"{col}: {n_missing}\")\n",
    "\n",
    "print(\"\\nUnique values in categorical columns:\")\n",
    "categorical_cols = ['leaf_shape_type', 'leaf_surface_type', 'petal_color']\n",
    "for col in categorical_cols:\n",
    "    unique_vals = named_df[col].unique()\n",
    "    print(f\"{col}: {list(unique_vals)}\")\n",
    "\n",
    "numeric_cols = [col for col in named_df.columns if col not in categorical_cols]\n",
    "plt.figure(figsize=(12, 6))\n",
    "named_df[numeric_cols].boxplot(showfliers=False)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "def count_outliers_iqr(series):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = series[(series < lower_bound) | (series > upper_bound)]\n",
    "    return len(outliers)\n",
    "\n",
    "n_outliers_total = 0\n",
    "for col in numeric_cols:\n",
    "    n_outliers = count_outliers_iqr(named_df[col])\n",
    "    n_outliers_total += n_outliers\n",
    "    print(f\"{col}: {n_outliers} outliers\")\n",
    "\n",
    "print(f\"Total outliers: {n_outliers_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qX3TQ7Te3Npz"
   },
   "outputs": [],
   "source": [
    "assert n_outliers_total == 3917"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5kAMK07olJ4"
   },
   "source": [
    "# üßº Task 4 (2 points): Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSgLtKv1drjG"
   },
   "source": [
    "The `snail_age` column certainly looks strange. Some data cleaning may be needed. After sending the initial analysis to SNAILAB researchers, one of them sent you an email with some additional information:\n",
    "\n",
    "________________\n",
    "\n",
    "*Thanks for flagging those issues in the dataset. Let me clarify some things.*\n",
    "\n",
    "*So the data came from several independent biologists working in different locations, and unfortunately they weren't all using the same units for snail age - some recorded it in years while others used days. You'll need to use a bit of common sense to figure out which is which, keeping in mind that some snails can live up to 10-15 years. For the cleaned dataset, we'd like everything standardized to years.*\n",
    "\n",
    "*Regarding the missing values, it's a bit nuanced. In some cases, a missing value means someone simply forgot to take that measurement, and we want those samples removed entirely from the analysis. But many missing values are actually meaningful because that measurement was impossible to get - please replace those with an appropriate constant like 0 or \"N/A\".*\n",
    "\n",
    "*One more thing about the body measurements: we measured body length from the tip of the snail's tail all the way to the tip of the right eye tentacle. Biologically speaking, the eye tentacles can't be longer than about 40% of the total body length, so if you're seeing values that exceed that, they're definitely measurement errors. We'd like you to replace those incorrect eye length values using 20% of the corresponding body length.*\n",
    "\n",
    "________________\n",
    "\n",
    "- (1.2p) According to this new information, get a new DataFrame called `clean_df` which **should not have any missing values** whatsoever. Keep `named_df` unchanged. For any removed or replaced missing values, add a comment with brief explanation why it was done.\n",
    "- (0.3p) Repair the eye length column.\n",
    "- (0.4p) Repair the snail age column. Plot two histograms for the snail age data, before and after, side by side. Increase the number of bins in plots for better resolution.\n",
    "- (0.1p) Repeat the boxplot from Task 3 but with `clean_df`.\n",
    "\n",
    "*Do not remove or replace outliers, leave them in the data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aF_dZe3Ooll0"
   },
   "outputs": [],
   "source": [
    "# your code\n",
    "clean_df = named_df.copy()\n",
    "\n",
    "print(\"Handling missing values:\")\n",
    "initial_missing = clean_df.isna().sum().sum()\n",
    "print(f\"Initial total missing values: {initial_missing}\")\n",
    "\n",
    "#Stulpeliai kur tu≈°ƒçios reik≈°mƒós pa≈°alinamos, nes tikƒótina pamir≈°ta pamatuoti.\n",
    "cols_to_check_removal = ['plant_height', 'stem_bending', 'leaf_size', 'flower_radius', \"snail_age\", \"snail_body_length\"]\n",
    "rows_before = len(clean_df)\n",
    "clean_df = clean_df.dropna(subset=cols_to_check_removal)\n",
    "rows_after = len(clean_df)\n",
    "print(f\"Removed {rows_before - rows_after} rows with missing basic plant measurements\")\n",
    "\n",
    "#Stulpeliai kur tu≈°ƒçios reik≈°mƒós pakeiƒçiamos ƒØ tam tikrƒÖ default, nes NaN tikƒótina turi biologinƒô reik≈°mƒô.\n",
    "clean_df['n_petals'] = clean_df['n_petals'].fillna(0)\n",
    "clean_df['petal_size'] = clean_df['petal_size'].fillna(0)\n",
    "clean_df['petal_color'] = clean_df['petal_color'].fillna('N/A')\n",
    "clean_df[\"snail_eye_length\"] = clean_df[\"snail_eye_length\"].fillna(0)\n",
    "print(\"Replaced missing petal measurements with 0 or 'N/A' (no flowers present) and missing snail eye length measurements with 0 (indicates that eyes were not visible to the researcher)\")\n",
    "\n",
    "#should not have any missing values whatsoever\n",
    "final_missing = clean_df.isna().sum().sum()\n",
    "print(f\"Final total missing values: {final_missing}\")\n",
    "\n",
    "\n",
    "print(\"\\nRepairing eye length measurements:\")\n",
    "def repair_eye_length(row):\n",
    "    body_length = row['snail_body_length']\n",
    "    eye_length = row['snail_eye_length']\n",
    "    \n",
    "    if eye_length > 0.4 * body_length:\n",
    "        return 0.2 * body_length\n",
    "    return eye_length\n",
    "\n",
    "clean_df['snail_eye_length'] = clean_df.apply(repair_eye_length, axis=1)\n",
    "invalid_eyes = ((named_df['snail_eye_length'] > 0.4 * named_df['snail_body_length']) & \n",
    "                named_df['snail_eye_length'].notna()).sum()\n",
    "print(f\"Repaired {invalid_eyes} biologically impossible eye length measurements\")\n",
    "\n",
    "\n",
    "print(\"\\nRepairing snail age column:\")\n",
    "def convert_to_years(age):\n",
    "    if age > 20:\n",
    "        return age / 365.25  #365.25 kad ƒØvertinti leap years\n",
    "    return age\n",
    "\n",
    "clean_df['snail_age'] = clean_df['snail_age'].apply(convert_to_years)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(named_df['snail_age'].dropna(), bins=100, color='red')\n",
    "plt.title('Snail Age Before Repair')\n",
    "plt.xlabel('Age (mixed units)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(clean_df['snail_age'], bins=100, color='green')\n",
    "plt.title('Snail Age After Repair (Years)')\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "converted_ages = (named_df['snail_age'] > 20).sum()\n",
    "print(f\"Converted {converted_ages} age values from days to years\")\n",
    "\n",
    "\n",
    "print(\"\\nNew boxplot for clean_df numeric columns:\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "clean_df[numeric_cols].boxplot(showfliers=False)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Numeric Columns (Clean Data)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SccW_4-7-T-"
   },
   "outputs": [],
   "source": [
    "assert list(named_df.columns) == list(clean_df.columns)\n",
    "assert len(named_df) > len(clean_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8EJjLOQomAR"
   },
   "source": [
    "# üñºÔ∏è Task 5 (3 points): Visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRfcIA4UgSQH"
   },
   "source": [
    "You were asked to create some beautiful EDA plots from `clean_df`. Create 3 plots in total. You will need to save each of them as a PDF image (filenames are provided), as well as output in the Notebook. Make sure everything is readable on the plot, no overlapping parts, cropped/unreadable texts, etc.\n",
    "\n",
    "1. (1p) **Feature distributions** (\"data_features.pdf\")\n",
    "   - A subplot grid (so, at least 2 rows or 2 columns), with as many subplots as there are features (columns) in the data.\n",
    "   - For each feature, plot either histogram or barplot depending on whether the feature is numeric (continuous/ranked) or categorical. The determination of feature type should be automatic, without manually writing column names as strings anywhere.\n",
    "   - For each numeric feature, add a red dashed vertical line indicating the average, and a green dashed vertical line for the median.\n",
    "   - Each histogram should have a title (feature name).\n",
    "   - Increase bin numbers for better resolution.\n",
    "   - Color all histograms in a color of your choice.\n",
    "\n",
    "2. (1p) **Feature correlations** (\"data_correlation.pdf\")\n",
    "   - A heatmap showing Spearman's correlation coefficients between all **numeric** features (reminder: you cannot use `seaborn`).\n",
    "   - The heatmap should have visible values (in [-100%, 100%] range, rounded), correct X and Y ticks (feature names), and a colorbar.\n",
    "   - Use a diverging colormap and ensure that 0% correlation has the middle color (check `vmin` and `vmax` parameters).\n",
    "   - The title of the plot should be \"Spearman correlation between features\".\n",
    "\n",
    "3. (1p) **Scatterplots of least and most correlated features** (\"data_relationships.pdf\")\n",
    "   - Two scatterplots side-by-side, first showing tho least correlated features, second - two most correlated.\n",
    "   - Least and most correlated features should be found automatically from the correlation matrix calculated for the second plot. Autocorrelation doesn't  count. Keep in mind that -100% is a huge correlation, just negative.\n",
    "   - Plots should have correctly named X and Y axes.\n",
    "   - Plot titles should contain Spearman correlation coefficients.\n",
    "   - Color dots according to the petal colors. Dot colors can be different from the color names used in data, but try to match them (except for the white color, obviously). Make dots smaller and more transparent than default ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttvIaUimomSc"
   },
   "outputs": [],
   "source": [
    "# your code\n",
    "\n",
    "n_features = len(clean_df.columns)\n",
    "n_cols = 4\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "plt.figure(figsize=(16, 4 * n_rows))\n",
    "\n",
    "for i, col in enumerate(clean_df.columns, 1):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    \n",
    "    if clean_df[col].dtype in ['float64', 'int64']:\n",
    "        plt.hist(clean_df[col], bins=100, color='skyblue', alpha=0.7)\n",
    "        plt.axvline(clean_df[col].mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "        plt.axvline(clean_df[col].median(), color='green', linestyle='--', linewidth=2, label='Median')\n",
    "    else:\n",
    "        value_counts = clean_df[col].value_counts()\n",
    "        plt.bar(value_counts.index, value_counts.values, color='lightcoral', alpha=0.7)\n",
    "        plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.title(col)\n",
    "    if i == 1 and clean_df[col].dtype in ['float64', 'int64']:\n",
    "        plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data_features.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "numeric_cols_for_corr = clean_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "correlation_matrix = clean_df[numeric_cols_for_corr].corr(method='spearman')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "im = plt.imshow(correlation_matrix * 100, cmap='RdBu_r', vmin=-100, vmax=100)\n",
    "\n",
    "for i in range(len(correlation_matrix)):\n",
    "    for j in range(len(correlation_matrix)):\n",
    "        plt.text(j, i, f'{correlation_matrix.iloc[i, j]*100:.0f}%', \n",
    "                ha='center', va='center', fontsize=8)\n",
    "\n",
    "plt.xticks(range(len(numeric_cols_for_corr)), numeric_cols_for_corr, rotation=45)\n",
    "plt.yticks(range(len(numeric_cols_for_corr)), numeric_cols_for_corr)\n",
    "plt.colorbar(im, label='Correlation (%)')\n",
    "plt.title('Spearman correlation between features')\n",
    "plt.tight_layout()\n",
    "plt.savefig('data_correlation.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "corr_pairs = []\n",
    "for i in range(len(correlation_matrix)):\n",
    "    for j in range(i + 1, len(correlation_matrix)):\n",
    "        corr_val = correlation_matrix.iloc[i, j]\n",
    "        corr_pairs.append((numeric_cols_for_corr[i], numeric_cols_for_corr[j], corr_val))\n",
    "\n",
    "corr_pairs_sorted = sorted(corr_pairs, key=lambda x: abs(x[2]))\n",
    "\n",
    "least_corr = corr_pairs_sorted[0] \n",
    "most_corr = corr_pairs_sorted[-1]  \n",
    "color_map = {\n",
    "    'white': 'lightgray',\n",
    "    'red': 'red', \n",
    "    'yellow': 'yellow',\n",
    "    'blue': 'blue',\n",
    "    'purple': 'purple',\n",
    "    'N/A': 'black'\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for color_name, color_code in color_map.items():\n",
    "    mask = clean_df['petal_color'] == color_name\n",
    "    plt.scatter(clean_df.loc[mask, least_corr[0]], \n",
    "                clean_df.loc[mask, least_corr[1]],\n",
    "                c=color_code, label=color_name, alpha=0.6, s=20)\n",
    "plt.xlabel(least_corr[0])\n",
    "plt.ylabel(least_corr[1])\n",
    "plt.title(f'Least correlated: r = {least_corr[2]:.3f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for color_name, color_code in color_map.items():\n",
    "    mask = clean_df['petal_color'] == color_name\n",
    "    plt.scatter(clean_df.loc[mask, most_corr[0]], \n",
    "                clean_df.loc[mask, most_corr[1]],\n",
    "                c=color_code, label=color_name, alpha=0.6, s=20)\n",
    "plt.xlabel(most_corr[0])\n",
    "plt.ylabel(most_corr[1])\n",
    "plt.title(f'Most correlated: r = {most_corr[2]:.3f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data_relationships.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Least correlated: {least_corr[0]} vs {least_corr[1]} (r = {least_corr[2]:.3f})\")\n",
    "print(f\"Most correlated: {most_corr[0]} vs {most_corr[1]} (r = {most_corr[2]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpXDpY4DiqSN"
   },
   "outputs": [],
   "source": [
    "import os  # do not delete this import\n",
    "assert os.path.exists(\"data_features.pdf\")\n",
    "assert os.path.exists(\"data_correlation.pdf\")\n",
    "assert os.path.exists(\"data_relationships.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMr94ZblwJYl"
   },
   "source": [
    "*Disclaimer: there is no lab called SNAILAB, and `plantdata.npy` is purely synthetic. Do not use this analysis to learn about snails. The data does not contain any information about the immortal snail. Do not use this analysis to evade the immortal snail. Evading the immortal snail is futile.*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
